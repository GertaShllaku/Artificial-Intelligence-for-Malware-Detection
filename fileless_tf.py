import matplotlib.pyplot as plt  
#Matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. 

import numpy as np  
#NumPy is a python library used for working with arrays.
#It also has functions for working in domain of linear algebra, fourier transform, and matrices.

import pandas as pd   
#pandas is a fast, powerful, flexible and easy to use #open source data analysis and manipulation tool, built on top of the Python programming language.

import sklearn  
#it's a free software machine learning library for the Python programming language.

import tensorflow as tf  
#An end-to-end open source machine learning platform #for everyone

from numpy import genfromtxt 
#Load data from a text file, with missing values handled as specified.

from sklearn import datasets  
#includes utilities to load datasets, including methods to load and fetch popular reference datasets.

from sklearn.metrics import (accuracy_score, confusion_matrix, f1_score,
                             precision_score, recall_score)

from sklearn.model_selection import train_test_split  #Split arrays or matrices #into random train and test subsets

from sklearn.preprocessing import LabelEncoder, StandardScaler #Encode target #labels with value between 0 and n_classes-1, Standardize features by removing the mean and scaling to unit variance

###############################################################

#Define the learning rate and number of epochs for AI network.

learning_rate = 0.001 # 0.1, 0.0001, etc
n_epochs = 5000
#more epochs

###############################################################
## tf.one_hot()

#Define the function convertOneHot() to perform one-hot encoding. it is often #used for indicating the state of a state machine.


def convertOneHot(data):
    y_onehot=[0]*len(data)
    for i,j in enumerate(data):
        y_onehot[i]=[0]*(data.max()+1)
        y_onehot[i][j]=1
    return y_onehot

###############################################################

#Read the features and class values from malware dataset with proper method
#log_file_features.csv is the name of the file
#delimiter indicates the character to split the data in a row.
#usecols indicates which columns will be read.For features the columns from 2 to #1000 will be read. For classes, the first columns of the rows will be read.
#dtype indicates the type of data to read


feature=genfromtxt('log_file_features.csv',delimiter=',', \
                           usecols=(i for i in range(1,1001)),dtype=int)
target=genfromtxt('log_file_features.csv',delimiter=',',usecols=(0), \
                                       dtype=int)

###############################################################

#Encode the labels for class values. Standard scale the feature values and split the dataset into training and testing set. Before splitting you need to perform one-hot encoding first.

sc = StandardScaler()
sc.fit(feature)
feature_normalized = sc.transform(feature)

#############################################################

target_label = LabelEncoder().fit_transform(target)
target_onehot = convertOneHot(target_label)

#############################################################

x_train, x_test, y_train_onehot, y_test_onehot = \
                    train_test_split(feature_normalized, target_onehot, \
                        test_size=0.25, random_state=0)

############################################################

#Define the parameters to store shape of the tensors to input

A=x_train.shape[1]
B=len(y_train_onehot[0])
print(A)   # features
print(B)   # columns
print("Begin:__________________________________")

###############################################################

#Define the function to print statistics metrics

precision_scores_list = []
accuracy_scores_list = []

def print_stats_metrics(y_test, y_pred):    
    print('Accuracy: %.2f' % accuracy_score(y_test,   y_pred) )
    
    accuracy_scores_list.append(accuracy_score(y_test,   y_pred) )
    confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)
    print ("confusion matrix")
    print(confmat)
    print (pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True))
    precision_scores_list.append(precision_score(y_true=y_test, y_pred=y_pred))
    print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred))
    print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))
    print('F1-measure: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))

#####################################################################


#Define the function to draw the plot to show the performance of the classifier at each epoch.

def plot_metric_per_epoch():
    x_epochs = []
    y_epochs = []
    for i, val in enumerate(accuracy_scores_list):
        x_epochs.append(i)
        y_epochs.append(val)

    plt.scatter(x_epochs, y_epochs,s=50,c='lightgreen', marker='s', label='score')
    plt.xlabel('epoch')
    plt.ylabel('score')
    plt.title('Score per epoch')
    plt.legend()
    plt.grid()
    plt.show()

###############################################################
##Deep Learning 
##############################################################

#Define the architecture of the artificial neural network

def layer(input, weight_shape, bias_shape):
    weight_stddev = (2.0/weight_shape[0])**0.5
    w_init = tf.random_normal_initializer(stddev=weight_stddev)
    bias_init = tf.constant_initializer(value=0)
    W = tf.get_variable("W", weight_shape, initializer=w_init)
    b = tf.get_variable("b", bias_shape, initializer=bias_init)
    return tf.nn.relu(tf.matmul(input, W) + b)

###############################################################

def inference_deep_layers(x_tf, n_features, n_columns):
    with tf.variable_scope("hidden_1"):
        hidden_1 = layer(x_tf, [n_features, 30],[30])
    with tf.variable_scope("hidden_2"):
        hidden_2 = layer(hidden_1, [30, 25],[25])
    with tf.variable_scope("hidden_3"):
        hidden_3 = layer(hidden_2, [25, 10],[10])
    with tf.variable_scope("hidden_4"):
        hidden_4 = layer(hidden_3, [10, 5],[5])
    with tf.variable_scope("output"):
        output = layer(hidden_4, [5, n_columns], [n_columns])
    return output

###############################################################

def loss_deep(output, y_tf):
    xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=output, \
                                                         labels=y_tf)
    loss = tf.reduce_mean(xentropy)
    return loss

###########################################################

def training(cost):
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    train_op = optimizer.minimize(cost)
    return train_op

###########################################################

def evaluate(output, y_tf):
    correct_prediction = tf.equal(tf.argmax(output,1), tf.argmax(y_tf,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
    return accuracy



# We defined the operations of each layer in layer() function
# We defined number of layers and neurons in each layer in #interference_deep_layers() function
#We calculate the cost in loss_deep() function
# We defined function to optimize and minimize the cost in training() function
# We defined the evaluate() function to evaluate the classifier

###############################################################


#Define the placeholders for iris datasets and input and output for each #function 

x_tf = tf.placeholder("float",[None,A]) # A=features
y_tf = tf.placeholder("float",[None,B]) # B=columns

###############################################################
#core function calls

output = inference_deep_layers(x_tf,A,B)
cost = loss_deep(output,y_tf)
train_op=training(cost)
eval_op=evaluate(output,y_tf)

###############################################################

init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)

###############################################################
# argmax returns the index of the max value for each row (dim=1)

y_p_metrics = tf.argmax(output,1)

###############################################################

#Perform the classification and print the statistics metrics. Finally, draw the  plots.

for i in range(n_epochs):
    print("epoch %s out of %s" % (i,n_epochs))
    sess.run(train_op,feed_dict={x_tf:x_train,y_tf:y_train_onehot})
    print ("------------------------------------------------------- \
                                            ------------------------")
    print ("Accuracy score")
    result, y_result_metrics = sess.run([eval_op, y_p_metrics], \
                     feed_dict={x_tf: x_test, y_tf: y_test_onehot})

    print("Run {},{}".format(i,result))

    y_true = np.argmax(y_test_onehot,1)
    y_pred = y_result_metrics

    print_stats_metrics(y_true, y_pred)

    if i==n_epochs-1:
        plot_metric_per_epoch()

################################################################

print("<<<<<DONE>>>>>>")
